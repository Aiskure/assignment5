# CLAUDE.md - Project Context for Claude Code

## ğŸ“ Learning Context

This is **Stanford CS336 Spring 2025 Assignment 5: Alignment** - a publicly available course assignment. The user is completing this assignment **for self-learning and skill development purposes**, not for academic credit or grade submission.

**Claude's Role**: ååŠ©è€…ï¼ˆtutorï¼‰ï¼Œè€Œéç›´æ¥å·¥ä½œè€…ã€‚æœ€ç»ˆç›®çš„æ˜¯æå‡ç”¨æˆ·èƒ½åŠ›ã€‚
- **ä¸è¦ç›´æ¥å†™å®Œæ•´å®ç°**ï¼Œè€Œæ˜¯å¼•å¯¼ç”¨æˆ·è‡ªå·±å®Œæˆ
- æä¾›æ€è·¯ã€æ¦‚å¿µè§£é‡Šã€ä¼ªä»£ç ã€å…³é”®æç¤º
- å½“ç”¨æˆ·å†™å¥½ä»£ç åï¼Œå¸®åŠ© review å’Œè°ƒè¯•
- ç”¨æˆ·å¡ä½æ—¶ï¼Œç»™å‡ºæœ€å°æç¤ºï¼ˆhintï¼‰ï¼Œè€Œéç­”æ¡ˆ
- å¯ä»¥æŒ‡å‡ºéœ€è¦ç”¨åˆ°çš„ API / å‡½æ•° / æ¨¡å¼ï¼Œä½†è®©ç”¨æˆ·è‡ªå·±ç»„è£…
- å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚"å¸®æˆ‘å†™"æ—¶ï¼Œæ‰ç›´æ¥å†™ä»£ç 

## Project Overview

This project implements LLM alignment techniques (SFT, Expert Iteration, GRPO) using Qwen 2.5 Math 1.5B.

**Important**: We use **GSM8K** instead of MATH as the training/evaluation dataset (MATH is not open-source).

## Repository Structure

```
assignment5/
â”œâ”€â”€ cs336_alignment/           # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ drgrpo_grader.py       # Math answer grading (r1_zero_reward_fn, question_only_reward_fn)
â”‚   â””â”€â”€ prompts/               # Prompt templates
â”‚       â”œâ”€â”€ r1_zero.prompt     # <think>/<answer> format (primary)
â”‚       â”œâ”€â”€ question_only.prompt
â”‚       â”œâ”€â”€ alpaca_sft.prompt
â”‚       â””â”€â”€ zero_shot_system_prompt.prompt
â”œâ”€â”€ tests/                     # Test suite
â”‚   â”œâ”€â”€ adapters.py            # *** MAIN FILE TO IMPLEMENT *** (all adapter functions)
â”‚   â”œâ”€â”€ conftest.py            # Pytest fixtures and snapshot testing utilities
â”‚   â”œâ”€â”€ common.py              # Shared constants (FIXTURES_PATH)
â”‚   â”œâ”€â”€ test_sft.py            # SFT-related tests
â”‚   â”œâ”€â”€ test_grpo.py           # GRPO-related tests
â”‚   â”œâ”€â”€ test_metrics.py        # MMLU/GSM8K parsing tests (optional)
â”‚   â”œâ”€â”€ test_data.py           # Dataset/dataloader tests (optional)
â”‚   â”œâ”€â”€ test_dpo.py            # DPO tests (optional)
â”‚   â””â”€â”€ _snapshots/            # Expected test outputs (.npz files)
â”œâ”€â”€ scripts/                   # Evaluation scripts
â”œâ”€â”€ plan.md                    # Detailed study plan (Chinese)
â”œâ”€â”€ å®éªŒè®°å½•.md                 # Experiment log
â”œâ”€â”€ å­¦ä¹ è®°å½•.md                 # Learning notes
â”œâ”€â”€ pyproject.toml             # Project config (uv, pytest)
â””â”€â”€ test_and_make_submission.sh # Test + zip for submission
```

## Key Information

- **Model**: Qwen 2.5 Math 1.5B at `/scratch/users/nus/e1553316/assignment5/models/Qwen2.5-Math-1.5B`
- **Dataset**: **GSM8K** (æ›¿ä»£ MATHï¼Œå›  MATH ä¸å¼€æº)
- **Package manager**: `uv` (not pip)
- **Python**: 3.11 or 3.12 (not 3.13)

## Training Environment (NSCC)

- **é›†ç¾¤**: NSCC (National Supercomputing Centre Singapore)
- **GPU**: 4x NVIDIA A100 40GB per job
- **æ€»æ˜¾å­˜**: 160GB per job
- **æ³¨æ„äº‹é¡¹**:
  - A100 40GB ç›¸æ¯” H100 æ˜¾å­˜è¾ƒå°ï¼Œéœ€è¦æ³¨æ„ micro_batch_size è®¾ç½®
  - 4 å¡å¹¶è¡Œæ—¶å¯ç”¨ `accelerate` æˆ– PyTorch DDP
  - å»ºè®® micro_batch_size=1~2 per GPUï¼Œç”¨ gradient accumulation å‡‘å¤§ batch
  - vLLM æ¨ç†æ—¶å¯ç”¨ `tensor_parallel_size=4` åŠ é€Ÿ rollout ç”Ÿæˆ

## Development Workflow

```bash
# Install dependencies
uv sync --no-install-package flash-attn && uv sync

# Run all tests
uv run pytest

# Run specific test
uv run pytest -k test_tokenize_prompt_and_output

# Run tests with verbose output
uv run pytest -v

# Generate submission
bash test_and_make_submission.sh
```

## Implementation Guide

All functions to implement are in `tests/adapters.py`. Each raises `NotImplementedError` by default. The actual implementations should go in `cs336_alignment/` and be imported by the adapters.

### Implementation Order (recommended)

**Phase 1 - SFT foundations:**
1. `run_tokenize_prompt_and_output` - tokenize prompt+output, build response_mask
2. `run_compute_entropy` - entropy of logits distribution
3. `run_get_response_log_probs` - forward pass â†’ per-token log probs
4. `run_masked_normalize` - sum and normalize with mask
5. `run_sft_microbatch_train_step` - SFT loss + backward for one microbatch

**Phase 2 - GRPO:**
6. `run_compute_group_normalized_rewards` - group-level advantage computation
7. `run_compute_naive_policy_gradient_loss` - basic REINFORCE loss
8. `run_compute_grpo_clip_loss` - PPO-style clipped loss
9. `run_compute_policy_gradient_loss` - dispatch wrapper
10. `run_masked_mean` - mean with mask
11. `run_grpo_microbatch_train_step` - GRPO loss + backward for one microbatch

**Phase 3 - Optional (RLHF/Safety):**
12. `get_packed_sft_dataset` - packed SFT dataset for instruction tuning
13. `run_iterate_batches` - batch iterator
14. `run_parse_mmlu_response` / `run_parse_gsm8k_response` - output parsing
15. `run_compute_per_instance_dpo_loss` - DPO loss

## Testing Details

- Tests use **snapshot testing** with `.npz` files in `tests/_snapshots/`
- Tolerances: `rtol=1e-4, atol=1e-2` (not exact match by default)
- Use `--snapshot-exact` flag for exact matching
- Fixtures are defined in `tests/conftest.py` with fixed random seeds (`torch.manual_seed(42)`)

## Important Patterns

- **Shifted labels**: `labels = input_ids[:, 1:]` with padding appended
- **Response mask**: 1 for response tokens in `labels`, 0 for prompt/padding
- **Gradient accumulation**: loss must be divided by `gradient_accumulation_steps` before `.backward()`
- **GRPO advantage**: computed per-group (G responses per prompt), normalized by group mean/std
- **Clip ratio**: `ratio = exp(log_prob - old_log_prob)`, clipped to `[1-eps, 1+eps]`

## Coding Conventions

- Type hints used throughout (`from __future__ import annotations`)
- PyTorch tensors for all numerical operations
- HuggingFace `transformers` for tokenizer and model
- `vllm` for efficient inference/rollout generation
- `wandb` for experiment tracking

## GSM8K Dataset Notes

- GSM8K æ˜¯å°å­¦æ•°å­¦åº”ç”¨é¢˜æ•°æ®é›†ï¼Œæ¯” MATH æ›´ç®€å•
- è®­ç»ƒé›† ~7.5K é¢˜ï¼Œæµ‹è¯•é›† ~1.3K é¢˜
- ç­”æ¡ˆæ ¼å¼ï¼šæœ€ç»ˆæ•°å­—ç­”æ¡ˆï¼ˆé€šå¸¸åœ¨ `#### <answer>` åï¼‰
- éœ€è¦é€‚é… reward function ä»¥åŒ¹é… GSM8K çš„ç­”æ¡ˆæ ¼å¼
- å¯ä»¥é€šè¿‡ HuggingFace Datasets è·å–ï¼š`datasets.load_dataset("gsm8k", "main")`

## NSCC Job Submission

```bash
# å…¸å‹çš„ PBS/SLURM job script ç»“æ„
#PBS -l select=1:ngpus=4
#PBS -l walltime=04:00:00
#PBS -q normal

# æˆ– SLURM
#SBATCH --gres=gpu:4
#SBATCH --time=04:00:00
```

## Latest Evaluation Status (2026-02-14)

- **Current baseline evaluation** has completed on GSM8K test split using `cs336_alignment/math_baseline.py`.
- **Result summary** (from `test_log.json`, n=1319):
  - accuracy: `0.0348749052` (~3.49%)
  - format_reward: `0.2934040940` (~29.34%)
  - type1/type2/type3: `46 / 341 / 932`
- **Output file**: `/scratch/users/nus/e1553316/assignment5/models/Qwen2.5-Math-1.5B/test_log.json`

## Evaluation Notes (Important)

- Use this command in project root:
  - `uv run python -m cs336_alignment.math_baseline`
- Do **not** use `uv python cs336_alignment/math_baseline.py` (invalid `uv` subcommand usage).
- `math_baseline.py` expects `data/gsm8k/test.jsonl` for evaluation input.
- In NSCC PBS jobs, `CUDA_VISIBLE_DEVICES` may be UUID-based (`GPU-...`). The script has been updated to normalize UUIDs to device indices before creating vLLM `LLM(...)`.
- `drgrpo_grader.py` may print `SyntaxWarning` for regex escape sequences; these warnings are non-fatal for current evaluation runs.

## Latest SFT Experiment Status (2026-02-16)

- **SFT training run** has completed with periodic vLLM evaluation on GSM8K test (`n=1319`).
- **Eval cadence**: every `1000` steps, checkpoints saved under `sft_eval/{step}` (and mirrored under `cs336_alignment/sft_eval/{step}`).
- **Final metrics** (step `22000`):
  - accuracy: `0.3282789992` (~32.83%)
  - type1/type2/type3: `433 / 606 / 280`
  - train/loss: `1.9609375`
  - train/grad_norm: `138`
- **Best checkpoint**: step `18000`, accuracy `0.3305534496` (~33.06%).
- **Outcome**: exceeds assignment target of at least `15%` validation accuracy on full dataset.

## W&B / Dependency Notes (2026-02-16)

- Cluster network/proxy may block online sync; current workflow uses **W&B offline** logs.
- Offline run directory:
  - `wandb/offline-run-20260216_001131-259v26t8`
- `wandb` dependency is pinned to:
  - `wandb==0.24.2` in `pyproject.toml`
- After dependency edits, run:
  - `uv lock`
  - `uv sync --no-install-package flash-attn`
